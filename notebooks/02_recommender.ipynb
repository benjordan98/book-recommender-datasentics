{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b846920-cf5f-436c-9db3-ea2f69954b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip\n",
    "\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ba463c4-16bb-42a0-9ce2-b0d9dc1d362c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37a267c0-0961-4cb0-b365-d8daa3b27edd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1001d17c-3691-457c-bfdb-53baa3be5c56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "books_df = pd.read_csv('../data/processed/books.csv')\n",
    "display(books_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f3f2629-4e82-4b62-85c5-e6244b3ba178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv('../data/processed/ratings.csv')\n",
    "display(ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aecec7a0-3601-4db3-8190-8eff6320a92e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "books_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03de93b8-97bc-410f-aff8-109245528696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is a lot of books and would produce a huge matrix for the cosine distances, especially when considering multiple features and embeddings or vectorisations of each book's features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94979ad2-5c4c-4042-86a9-44896e86992e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Popular books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d41b4624-84d9-4db6-91f8-3d3ce33ae21e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Due to Google API limits (which I plan to use for further augmentation) I will take the top 1000 books from the dataset according to number of reviews in the reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4735e4f9-cbaa-4b59-8f5b-b6be5a09b958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ratings_count = ratings_df.groupby('ISBN').size().reset_index(name='num_ratings')\n",
    "books_df = books_df.merge(ratings_count, on='ISBN', how='left')\n",
    "top_books = books_df.sort_values(by='num_ratings', ascending=False).head(1000)\n",
    "top_books.drop(columns=['num_ratings'], inplace=True)\n",
    "top_books.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c7e656-e248-4301-964d-e0a5067fc58f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lets add some more features\n",
    "\n",
    "For my content based recommender just using author, title, publisher and age is not going to be enough to get good predictions. I will use Google Books API to augment the data to get additional features such as genre, page count, description and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d916e0c7-0b82-43d8-8ce1-893234ec78a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def get_google_books_data_no_key(title, author):\n",
    "    query = f\"intitle:{title}+inauthor:{author}\"\n",
    "    url = f\"https://www.googleapis.com/books/v1/volumes?q={query}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if data['totalItems'] == 0:\n",
    "            return None\n",
    "\n",
    "        book_info = data['items'][0]['volumeInfo']\n",
    "        return {\n",
    "            'description': book_info.get('description'),\n",
    "            'categories': book_info.get('categories'),\n",
    "            'pageCount': book_info.get('pageCount'),\n",
    "            'averageRating': book_info.get('averageRating'),\n",
    "            'ratingsCount': book_info.get('ratingsCount'),\n",
    "            'language': book_info.get('language')\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for '{title}' by {author}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_google_books_data(title, author):\n",
    "    api_key = \"MY_KEY_PLACEHOLDER\"\n",
    "    query = f\"intitle:{title}+inauthor:{author}\"\n",
    "    url = f\"https://www.googleapis.com/books/v1/volumes?q={query}&key={api_key}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if data['totalItems'] == 0:\n",
    "            return None\n",
    "\n",
    "        book_info = data['items'][0]['volumeInfo']\n",
    "        return {\n",
    "            'description': book_info.get('description'),\n",
    "            'categories': book_info.get('categories'),\n",
    "            'pageCount': book_info.get('pageCount'),\n",
    "            'averageRating': book_info.get('averageRating'),\n",
    "            'ratingsCount': book_info.get('ratingsCount'),\n",
    "            'language': book_info.get('language')\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for '{title}' by {author}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72141611-9fbe-4056-93fd-78a93db15dd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### All commented out code from the API request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf385231-5b65-4f74-87ea-89ddf4f62128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# augmented_books_data = []\n",
    "\n",
    "# for index, row in tqdm(top_books.iterrows()):\n",
    "#     book_info = get_google_books_data(row['bookTitle'], row['bookAuthor'])\n",
    "#     if book_info:\n",
    "#         book_info['bookTitle'] = row['bookTitle']\n",
    "#         book_info['bookAuthor'] = row['bookAuthor']\n",
    "#         augmented_books_data.append(book_info)\n",
    "#     else:\n",
    "#         print(f\"No book data found for '{row['bookTitle']}' by {row['bookAuthor']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2df743b-b22d-441b-80b2-700b0ea71ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# top_books[top_books.bookTitle == 'the devil in the white city : murder, magic, and madness at the fair that changed america (illinois)'].index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33352d59-f0b4-40cb-8b5f-5753edc9c558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I ran out of requests at index 945 so I'll be cheeky and do the rest without an api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90864fbc-c0fc-43a5-a743-c975b220be71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ran out of requests at index 945 do the rest without an api key\n",
    "# for index, row in tqdm(top_books.iloc[945:].iterrows()):\n",
    "#     book_info = get_google_books_data_no_key(row['bookTitle'], row['bookAuthor'])\n",
    "#     if book_info:\n",
    "#         book_info['bookTitle'] = row['bookTitle']\n",
    "#         book_info['bookAuthor'] = row['bookAuthor']\n",
    "#         augmented_books_data.append(book_info)\n",
    "#     else:\n",
    "#         print(f\"No book data found for '{row['bookTitle']}' by {row['bookAuthor']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c445047-ba98-4bb2-b83c-171804eb1f25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I drop `averageRating` and `ratingsCount` high null percentage and can get similar from `ratings_df` anyways. `pageCount` has a lot of 0 values which are likely missing so I'll replace with null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fba5b474-7c06-4bbc-8e5b-9bf114b11ac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# augmented_df = pd.DataFrame(augmented_books_data)\n",
    "# augmented_df = augmented_df.drop(['averageRating', 'ratingsCount', 'language'], axis=1)\n",
    "# augmented_df['pageCount'] = augmented_df['pageCount'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f321857-35ec-4d1b-adbc-f1c77c158756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # check for duplicates based on bookTitle and bookAuthor\n",
    "# duplicates = augmented_df.duplicated(subset=['bookTitle', 'bookAuthor'])\n",
    "# print(f\"Number of duplicate entries: {duplicates.sum()}\")\n",
    "# # drop duplicates\n",
    "# augmented_df = augmented_df.drop_duplicates(subset=['bookTitle', 'bookAuthor'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e4d80dc-88ba-48f1-9051-34ff45480d6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "# augmented_df.to_csv(\"../data/external/augmented_books_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a8774fe-c31a-4266-8c97-ba7ef1fd1b0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Process the Google books data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55af3a6a-3164-4ce7-bde0-f1169888deb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "augmented_df = pd.read_csv(\"../data/external/augmented_books_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ac7da8a-9ea9-41aa-9fe6-351c1ded8d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "augmented_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d65a0356-a9a7-47eb-9918-081801ca4601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Unfortunately there is some missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab788e48-b6f3-4c97-ba5d-c99813fb2e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "augmented_df[augmented_df.description.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1747a0db-0608-40e4-86ed-3485d29eab2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "It appears that even some very popular books are missing - this is likely due to small differences in the `bookTitle` or `author` in the text or some illegal punctuation in the request URL due to the author or title. I also just realised that I can use the ISBN with google api - next time I would definitely go for that option. But for now I'm out of requests and there are not too many missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9757bc51-2908-434d-8813-da5966ec5bca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Close your eyes while I shamefully drop all rows with missing entries for simiplicity. Even if I could not get the information using the API with ISBN I could:\n",
    "- Impute the `pageCount` using the median/mean potentially stratified by author and/or publisher\n",
    "- Impute the `categories` (actually just one category) again using the publisher\n",
    "- Leave the `description` empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "110b844a-bb55-4e1c-ad6a-f31509d78797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_books = top_books.merge(augmented_df, on=['bookTitle', 'bookAuthor'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fe456b3-e5d2-4e3b-9119-3b7ee831554d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_books = top_books.dropna()\n",
    "top_books = top_books.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0931de26-4d7a-4acc-b3ec-3189ac58656f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# convert categories to list\n",
    "import ast\n",
    "top_books['categories'] = top_books['categories'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef39838b-6db7-4bbb-99db-be76d664e659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_books['categories'].apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dedc7bf2-956f-41a8-9ba8-09a600ce99f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_books['category'] = top_books['categories'].apply(lambda x: x[0] if x else None)\n",
    "top_books.drop(columns=['categories'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f55b33b1-f009-4af9-87e1-d60f1f3dc8ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_books['category'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7062a2e7-1312-47ac-ac69-dcecdeab5ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rather disappointingly there is only one category per book, I saw in plenty of datasets online lists of 10s of categories for each book. This would provide a lot more information, I would try to get more genre information in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fceb2067-d5fc-4ba9-9673-00e0542a82f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_books['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff43f84c-ea5e-4dda-82a4-fd71500ac4e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "However for now this makes my life a lot simpler since I'm not doing any sophisticated embeddings and simpler TF-IDF vectorisation will likely not help too much since the categories are very short and likely won't contain similar words for similar genres and in some cases could be confusing Fiction vs Non-Fiction. I will likely go for a very simple distance metric from one hot-encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e9f021-2a95-4bc2-99b4-6e4becb7446a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# First Content Based Recommender\n",
    "\n",
    "I will start with a recommender that only uses the data provided in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ad07a3f-80ea-49b8-9cca-de8fa828f9d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ddce98d-34b8-467a-bc40-c262d039e8e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Base Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd5d44c-9daf-472a-832a-51fa95a39f9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "author_encoded = pd.get_dummies(top_books['bookAuthor'], prefix='author')\n",
    "publisher_encoded = pd.get_dummies(top_books['publisher'], prefix='publisher')\n",
    "age_normalised = MinMaxScaler().fit_transform(top_books[['bookAge']])\n",
    "x_auth_pub_age = pd.concat([author_encoded.reset_index(drop=True), publisher_encoded.reset_index(drop=True), pd.DataFrame(age_normalised).reset_index(drop=True)], axis=1)\n",
    "similarity_matrix = cosine_similarity(x_auth_pub_age)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0510e931-1692-4089-ae5b-55dc14bee14a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def recommend(book_index, similarity_matrix, df, top_n=5):\n",
    "    sim_scores = list(enumerate(similarity_matrix[book_index]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    top_indices = [i for i, _ in sim_scores[1:top_n+1]]\n",
    "    recommendations = df.iloc[top_indices].drop(columns=['ISBN'])\n",
    "    recommendations['Score'] = [score for _, score in sim_scores[1:top_n+1]]\n",
    "    recommendations['Rank'] = range(1, len(recommendations) + 1)\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec0304a1-fcb2-4681-ad3b-35e215ee812b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "book_index = 1\n",
    "print(f\"Book : {top_books.iloc[book_index]}\")\n",
    "recommend(book_index, similarity_matrix, top_books, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2a61372-4f2a-4532-9b8b-1bc3d23546c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As a little qualitative test I can see that the author and the publisher ar ethe same and the book age similar, suggesting that the similarity matrix is working as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de08fcbb-2315-48b8-9513-664f00be3970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Encode book title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc8f051-1c3a-4ba7-b61c-86c61d9d8f6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For now I will start with TF-IDF vectorisations as they're simple to implement and still relevant although not as powerful as semamtic embeddings from BERT or learned embeddings. Similar books can still often contain the some of the same words so this could still be valuable information.\n",
    "\n",
    "Although often book titles are quite abstract and contain the same words and be very differnt. The same can be said for book titles with similar semantic meanings too such as 'Three comrades' and 'Conversations with friends'!\n",
    "\n",
    "So clearly more than just the book title would be needed to recommend another book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c796d99-eaad-4102-8717-c793d8e6ab33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix_title = tfidf.fit_transform(top_books['bookTitle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b34fedf-f58f-453c-b1d6-7493dad2a3d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "X_sparse = csr_matrix(x_auth_pub_age)\n",
    "combined_features = hstack([tfidf_matrix_title, X_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f02d475a-24c3-4e03-8639-d21894f85476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "similarity_matrix_with_title = cosine_similarity(combined_features)\n",
    "book_index = 2\n",
    "print(f\"Book : {top_books.iloc[book_index]}\")\n",
    "recommend(book_index, similarity_matrix_with_title, top_books, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c685e88-fa7e-4232-872d-e8595e623e80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here you can see the `bookTitle` is clearly being used as information to suggest similar books `a novel` is repeated in the top 4 suggestions. This is clearly not the most sophisticated recommendation of just suggesting other books with `a novel` however it is implicitly suggesting other fiction books just from the title without even providing the additional gender information! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ac7ca3-a8da-4a4c-bde2-0732e98452e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## How can I make these predictions interpretable?\n",
    "\n",
    "A key goal and reason I went for the content-based recommender is because I wanted to add an interpetable aspect to the prediction, which I think for books is especially valuable and a feature I haven't seen before.\n",
    "\n",
    "This also opens the door to incorporate feedback from the user on the suggestion on specific metrics.\n",
    "\n",
    "The seemingly straightforward solution is to decompose the similarity matrix considering each factor separately and the combining them with weights. The next question is what should these weights be. The weights could be learned or since I am also keen on incorporating user feedback for this first step I can make these parameters which the user can directly adjust. In later stages the feature importance could be learned a more sophisticated user input mechanism introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "946a1946-26a9-4b6e-8ac5-68f41b271e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def inverse_normalised_distance_similarity(vec):\n",
    "    vec = np.array(vec).reshape(-1, 1)  # Ensure column vector\n",
    "    dists = np.abs(vec - vec.T)\n",
    "    max_dist = dists.max()\n",
    "\n",
    "    if max_dist == 0:\n",
    "        # All values are identical â†’ full similarity\n",
    "        return np.ones_like(dists)\n",
    "\n",
    "    similarity = 1 - (dists / max_dist)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067bfd30-534c-4a19-a7d8-d5599a88b520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For scalars I use an inverse normalised manhatten distance as a first metric. I only have `bookAge` and later also `pageCount`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19bb8ec8-e268-4641-8af5-9adb701a9888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "author_array = np.array(top_books['bookAuthor'])\n",
    "similarity_authors = np.equal.outer(author_array, author_array).astype(float)\n",
    "publisher_array = np.array(top_books['publisher'])\n",
    "similarity_publishers = np.equal.outer(publisher_array, publisher_array).astype(float)\n",
    "tfidf_title_matrix = tfidf.fit_transform(top_books['bookTitle'])\n",
    "similarity_title = cosine_similarity(tfidf_title_matrix)\n",
    "age_array = np.array(top_books['bookAge'])\n",
    "similarity_age = inverse_normalised_distance_similarity(age_array)\n",
    "\n",
    "similarity_weights = {'title': 0.3, 'author': 0.3, 'publisher': 0.3, 'age': 0.1}\n",
    "similarity_matrices = {\n",
    "    'title': similarity_title,\n",
    "    'author': similarity_authors,\n",
    "    'publisher': similarity_publishers,\n",
    "    'age': similarity_age\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd93d16-a93a-4911-9a49-1e164985b21e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def recommend_books_interpretable(book_index, similarity_matrices, weights, df, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommend books using interpretable similarity components.\n",
    "    \n",
    "    Parameters:\n",
    "        book_index (int): Index of the reference book\n",
    "        similarity_matrices (dict): Dict of feature name -> similarity matrix (NxN)\n",
    "        weights (dict): Dict of feature name -> weight (must match keys in similarity_matrices)\n",
    "        df (pd.DataFrame): DataFrame with book info\n",
    "        top_n (int): Number of recommendations to return\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Recommended books with similarity breakdown and total score\n",
    "    \"\"\"\n",
    "    feature_scores = {}\n",
    "    \n",
    "    # Extract similarity for the target book for each feature\n",
    "    for feature, sim_matrix in similarity_matrices.items():\n",
    "        feature_scores[feature] = sim_matrix[book_index]\n",
    "\n",
    "    # Compute final weighted similarity score\n",
    "    final_similarity = np.zeros_like(next(iter(feature_scores.values())))\n",
    "    for feature in feature_scores:\n",
    "        final_similarity += weights[feature] * feature_scores[feature]\n",
    "\n",
    "    # Enumerate and rank by final score\n",
    "    sim_scores = list(enumerate(final_similarity))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    top_indices = [i for i, _ in sim_scores if i != book_index][:top_n]\n",
    "\n",
    "    # Build result dataframe\n",
    "    recommendations = df.iloc[top_indices].drop(columns=['ISBN'])\n",
    "    recommendations['Rank'] = range(1, len(recommendations) + 1)\n",
    "\n",
    "    # Add feature-wise similarity scores\n",
    "    for feature in feature_scores:\n",
    "        recommendations[f'sim_{feature}'] = [feature_scores[feature][i] for i in top_indices]\n",
    "\n",
    "    # Add final similarity score\n",
    "    recommendations['Score'] = [final_similarity[i] for i in top_indices]\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64fbd368-5e46-4456-9b3f-8e1d606635db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "similarity_matrix_with_title = cosine_similarity(combined_features)\n",
    "book_index = 4\n",
    "print(f\"Book : {top_books.iloc[book_index]}\")\n",
    "recommend(book_index, similarity_matrix_with_title, top_books, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8170c3c8-1b41-44b7-b8bf-d4f90f22378d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "book_index = 4\n",
    "print(f\"Book : {top_books.iloc[book_index]}\")\n",
    "similarity_weights = {'title': 0.4, 'author': 0.3, 'publisher': 0.2, 'age': 0.1}\n",
    "recommend_books_interpretable(book_index, similarity_matrices, similarity_weights, top_books, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c1772d-32d8-4114-9787-536cd6130906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The suggests are comparable with one difference in the first suggestion, which you can see is suggested due to similarity in title, however the suggestions are still overwhelminginly dominated by books from ballentine books publisher. It does not seem like this is enough information to really make such a good recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c143c7e-cd24-4fe0-ab73-c144aec33da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Add in Google Books Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d820bc-0b05-4780-887c-73ac46917aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "On top of author, publisher, title and age. I will add description, category and the page count of the books as factors to be used in comparing the books. \n",
    "\n",
    "- For `description` I'll use TF-IDF vectorisation once again capturing semantics would be a lot more powerful here.\n",
    "- For `category` since it is only one category I will just exactly compare categories rather than through an encoding. I would be interesting to see how differently an embedding would behave as I did notice categories such as `Fiction` and `Juvenile Fiction` which an embedding would be able to capture this similarity.\n",
    "- For `pageCount` I used the scalar metric as before with age. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852969bd-07b8-4c39-822f-e3c1b24bda36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tfidf_matrix_description = tfidf.fit_transform(top_books['description'])\n",
    "similarity_matrix_description = cosine_similarity(tfidf_matrix_description)\n",
    "category_array = np.array(top_books['category'])\n",
    "similarity_matrix_category = np.equal.outer(category_array, category_array).astype(float)\n",
    "pages_array = np.array(top_books['pageCount'])\n",
    "similarity_pages = inverse_normalised_distance_similarity(pages_array)\n",
    "# also for other features\n",
    "author_array = np.array(top_books['bookAuthor'])\n",
    "similarity_authors = np.equal.outer(author_array, author_array).astype(float)\n",
    "publisher_array = np.array(top_books['publisher'])\n",
    "similarity_publishers = np.equal.outer(publisher_array, publisher_array).astype(float)\n",
    "tfidf_title_matrix = tfidf.fit_transform(top_books['bookTitle'])\n",
    "similarity_title = cosine_similarity(tfidf_title_matrix)\n",
    "age_array = np.array(top_books['bookAge'])\n",
    "similarity_age = inverse_normalised_distance_similarity(age_array)\n",
    "\n",
    "similarity_weights_extra = {'title': 0.1, 'author': 0.2, 'publisher': 0.1, 'age': 0.05, 'description': 0.2, 'category': 0.2, 'pages': 0.05}\n",
    "\n",
    "similarity_matrices_extra = {\n",
    "    'title': similarity_title,\n",
    "    'author': similarity_authors,\n",
    "    'publisher': similarity_publishers,\n",
    "    'age': similarity_age,\n",
    "    'description': similarity_matrix_description,\n",
    "    'category': similarity_matrix_category,\n",
    "    'pages': similarity_pages\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce4689af-35be-43cb-8f94-b300b6257559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "book_index = 4\n",
    "print(f\"Book : {top_books.iloc[book_index]}\")\n",
    "similarity_weights_extra = {'title': 0.1, 'author': 0.2, 'publisher': 0.1, 'age': 0.05, 'description': 0.2, 'category': 0.2, 'pages': 0.05}\n",
    "recommend_books_interpretable(book_index, similarity_matrices_extra, similarity_weights_extra, top_books, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aa76859-a0b1-4546-aec2-b9277c4f8048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The recommendations are different to what I had before so the additional features and also slighlty different weights are playing a role. I can also see the description similarities are often very low likely because I'm not capturing any semantic meaning and the overlap in words is likely small. Its probably wise if this is given more weight than the other categories especially since people tend to pick books based on descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c212c198-31f6-4f2a-9f23-39f1c10cf6d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cd8e8ae-fd93-4206-8e32-6999d4a1fcb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I will use the users ratings to evaluate my model. Since my setting is a little uncoventional, I need a slightly different way to evaluate it. I flip regular LOOCV on its head.\n",
    "\n",
    "I iterate over the set of books the user has read and enjoyed. I take one as the test book which I actually use as my input to the algorithm and compare the set of predictions to the rest of the books the user has read and enjoyed. I calculate average hit rate, average precision@N, average recall@N for each model.\n",
    "\n",
    "I have been tinkering with these metrics a bit regarding the fairest ways to record them especially with recall. Open for discussion!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70a448e9-6082-4885-9d9d-c1dc7237ccce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv('../data/processed/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06dd5104-0fa6-4c1a-bcc0-5570137c89b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filter ratings to match top_books\n",
    "ratings_df = ratings_df[ratings_df['ISBN'].isin(top_books['ISBN'])]\n",
    "ratings_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a10f72b-ae8e-47be-89cd-1638509ff4db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Even with the reduced amount of books there are still plenty of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8fceca5-e283-49d9-b6da-cf065c1d6bfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "min_ratings_per_user = 50\n",
    "user_counts = ratings_df['userID'].value_counts()\n",
    "active_users = user_counts[user_counts >= min_ratings_per_user].index\n",
    "ratings_filtered = ratings_df[ratings_df['userID'].isin(active_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80f146d8-d2eb-4769-89e5-8abe4d9f7b1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I give a rather low threshold on the book ratings to preserve my sample size and use the threshold of ratings greater than 5 to be relevant/positively reviewed books. This does mean for now I have removed implicit reviews entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56b52a86-a0ff-45c8-b449-4b233aad213a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ratings_filtered.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "826f4b99-f3be-4724-b4c6-73bf8791729a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After filtering to find users that have actually reviewed over 50 books I'm left with just over 30000 which is not so many books anymore but it is important for the evaluation that the users have read and reviewed at least a few books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b34848-e336-43e0-8de8-a49f2bd923b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ratings_filtered[ratings_filtered['bookRating'] > 5].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f7562d-1832-4135-b977-7683b220cee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note though that most of these are implicit and some negative (less than 5) reviews which won't be in the target set later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd93b91-0c18-43b5-b70a-0bead52c6f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ratings_positive = ratings_filtered[ratings_filtered['bookRating'] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ce37b9-5cb1-4b14-b835-e85c179274d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Count ratings\n",
    "user_counts_all = ratings_filtered['userID'].value_counts()\n",
    "user_counts_positive = ratings_positive['userID'].value_counts()\n",
    "\n",
    "# Plot overlaid histograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(user_counts_all, bins=50, alpha=0.5, label='All Ratings')\n",
    "plt.hist(user_counts_positive, bins=50, alpha=0.5, label='Positive Ratings')\n",
    "\n",
    "plt.xlabel('Number of Ratings per User')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Ratings per User')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2e60e51-150b-4792-bb29-d9dd3d7e4d87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ratings_positive.groupby('userID').size().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a259ec3-0be3-457e-a4f6-c596bec39ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ratings_positive.ISBN.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d262eb0-bdcf-4241-80b4-06df6e597cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ratings_filtered.ISBN.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa771eef-c7ae-4a11-be94-0a0d3bf151e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The expectedly relatively low proportion of total books that the average reader has review positively means I should brace for rather low scores. The similarity matrices consider 844 books and the ratings only have 818 positively reviewed books and the average user has read and reviewd 20 of those popularly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69dc70b6-8650-4747-864d-65911e986172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ratings_filtered.userID.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28951700-5628-4495-bd1a-9cd0685c3fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We are still evaluating on 300 users too. This is a large enough sample size to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e38d45-243b-4e2e-84c6-c969a44f34a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def recommend_indices(book_index, similarity_matrix, df, top_n=5):\n",
    "    sim_scores = list(enumerate(similarity_matrix[book_index]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    top_indices = [i for i, _ in sim_scores[1:top_n+1]]\n",
    "    return top_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d6ceeec-0f35-499a-8957-33b632a2b3ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c51fb002-5342-4cfd-b19d-9f714926871b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def recommend_popular_book_indices(books_df, ratings_df, top_n=5):\n",
    "    \"\"\" recommend the book with simply the greatest number of reviews\"\"\"\n",
    "    book_counts = ratings_df.groupby('ISBN')['bookRating'].count().reset_index()\n",
    "    book_counts.columns = ['ISBN', 'count']\n",
    "    top_books = books_df.merge(book_counts, on='ISBN')\n",
    "    top_books = top_books.sort_values(by='count', ascending=False)\n",
    "    top_books = top_books.head(top_n)\n",
    "    # return indices\n",
    "    return top_books.index.tolist()\n",
    "    \n",
    "recommend_popular_book_indices(top_books, ratings_filtered)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73cd6160-9404-487a-aacb-7afc8d6f7eae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_books.iloc[recommend_popular_book_indices(top_books, ratings_filtered)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1db4ea3-9771-4af9-8ac0-e7a47a849afb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "These seem to be popular books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b010cc0c-1f11-4cb1-9575-e7c3c750eccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "hits = 0\n",
    "total = 0\n",
    "precision_total = 0\n",
    "recall_total = 0\n",
    "users_less_than_6_pos_ratings = 0\n",
    "\n",
    "recommended_book_indices = recommend_popular_book_indices(top_books, ratings_filtered, top_n) \n",
    "\n",
    "for user, group in tqdm(ratings_filtered.groupby('userID')):\n",
    "    user_books = group[group['bookRating'] >= 5]['ISBN'].tolist()\n",
    "    if len(user_books) < 6:\n",
    "        users_less_than_6_pos_ratings += 1\n",
    "        continue\n",
    "\n",
    "    for seed_book in user_books:\n",
    "        other_liked_books = [b for b in user_books if b != seed_book]\n",
    "        if not other_liked_books:\n",
    "            continue\n",
    "\n",
    "        # get ISBN of recommended books\n",
    "        recommended_books = top_books.loc[recommended_book_indices, 'ISBN'].tolist()\n",
    "        # True positives\n",
    "        relevant_recs = set(recommended_books).intersection(set(other_liked_books))\n",
    "        hits += int(len(relevant_recs) > 0)\n",
    "        precision_total += len(relevant_recs) / top_n\n",
    "        recall_total += len(relevant_recs) / len(other_liked_books)\n",
    "        total += 1\n",
    "\n",
    "# Metrics\n",
    "print(f\"Hit Rate@{top_n}: {hits / total:.4f}\")\n",
    "print(f\"Precision@{top_n}: {precision_total / total:.4f}\")\n",
    "print(f\"Recall@{top_n}: {recall_total / total:.4f}\")\n",
    "\n",
    "hit_rate_baseline = hits / total\n",
    "precision_baseline = precision_total / total\n",
    "recall_baseline = recall_total / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d85b3a-2075-460d-9ea3-7b5b3bbcd1e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After realising how I would calculate the metrics I noticed there could still be users without enough, 6 positive ratings there turned out to be 49 of the 338 I was using in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32a75298-bc01-49ec-b046-a320fee42eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Users still in ratings with less than 6 positive ratigns: {users_less_than_6_pos_ratings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ac6cee-b8dc-4243-a026-d761b4e40b80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656e3b20-168c-4b85-b86f-fd1a18b324ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "hits = 0\n",
    "total = 0\n",
    "precision_total = 0\n",
    "recall_total = 0\n",
    "\n",
    "for user, group in tqdm(ratings_filtered.groupby('userID')):\n",
    "    user_books = group[group['bookRating'] >= 5]['ISBN'].tolist()\n",
    "    if len(user_books) < 6:\n",
    "        continue\n",
    "\n",
    "    for seed_book in user_books:\n",
    "        other_liked_books = [b for b in user_books if b != seed_book]\n",
    "        if not other_liked_books:\n",
    "            continue\n",
    "        \n",
    "        book_index = top_books[top_books['ISBN'] == seed_book].index[0]\n",
    "        recommended_books = recommend_indices(book_index, similarity_matrix, top_books, top_n)\n",
    "        # get ISBN of recommended books\n",
    "        recommended_books = top_books.loc[recommended_books, 'ISBN'].tolist()\n",
    "        # True positives\n",
    "        relevant_recs = set(recommended_books).intersection(set(other_liked_books))\n",
    "        hits += int(len(relevant_recs) > 0)\n",
    "        precision_total += len(relevant_recs) / top_n\n",
    "        recall_total += len(relevant_recs) / len(other_liked_books)\n",
    "        total += 1\n",
    "\n",
    "# Metrics\n",
    "print(f\"Hit Rate@{top_n}: {hits / total:.4f}\")\n",
    "print(f\"Precision@{top_n}: {precision_total / total:.4f}\")\n",
    "print(f\"Recall@{top_n}: {recall_total / total:.4f}\")\n",
    "\n",
    "hit_rate_base_features = hits / total\n",
    "precision_base_features = precision_total / total\n",
    "recall_base_features = recall_total / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f14d05e2-0ce3-490f-b479-4b703d78a715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Phew that's a relief my model is better than the baseline model just."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d3ae80-070c-42bf-9f09-45a3d66dd930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Now with title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcc07eb3-5eff-48cc-8de0-a0bcc4aeaa39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "hits = 0\n",
    "total = 0\n",
    "precision_total = 0\n",
    "recall_total = 0\n",
    "\n",
    "for user, group in tqdm(ratings_filtered.groupby('userID')):\n",
    "    user_books = group[group['bookRating'] >= 5]['ISBN'].tolist()\n",
    "    if len(user_books) < 6:\n",
    "        continue\n",
    "\n",
    "    for seed_book in user_books:\n",
    "        other_liked_books = [b for b in user_books if b != seed_book]\n",
    "        if not other_liked_books:\n",
    "            continue\n",
    "        \n",
    "        book_index = top_books[top_books['ISBN'] == seed_book].index[0]\n",
    "        recommended_books = recommend_indices(book_index, similarity_matrix_with_title, top_books, top_n)\n",
    "        # get ISBN of recommended books\n",
    "        recommended_books = top_books.loc[recommended_books, 'ISBN'].tolist()\n",
    "        # True positives\n",
    "        relevant_recs = set(recommended_books).intersection(set(other_liked_books))\n",
    "        hits += int(len(relevant_recs) > 0)\n",
    "        precision_total += len(relevant_recs) / top_n\n",
    "        recall_total += len(relevant_recs) / len(other_liked_books)\n",
    "        total += 1\n",
    "\n",
    "# Metrics\n",
    "print(f\"Hit Rate@{top_n}: {hits / total:.4f}\")\n",
    "print(f\"Precision@{top_n}: {precision_total / total:.4f}\")\n",
    "print(f\"Recall@{top_n}: {recall_total / total:.4f}\")\n",
    "\n",
    "hit_rate_with_title = hits / total\n",
    "precision_with_title = precision_total / total\n",
    "recall_with_title = recall_total / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3945820b-0fcc-4a65-ab64-b5decff6141a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Including the title does marginally improve the performance on both precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f0c7c79-7180-473a-a6e8-f21ca852c644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Genre, description ... model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0d156e1-26a4-4852-9463-c1ed4c9d41b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def recommend_indices_intepretable(book_index, similarity_matrices, weights, df, top_n=5):\n",
    "    feature_scores = {}\n",
    "    \n",
    "    # Extract similarity for the target book for each feature\n",
    "    for feature, sim_matrix in similarity_matrices.items():\n",
    "        feature_scores[feature] = sim_matrix[book_index]\n",
    "\n",
    "    # Compute final weighted similarity score\n",
    "    final_similarity = np.zeros_like(next(iter(feature_scores.values())))\n",
    "    for feature in feature_scores:\n",
    "        final_similarity += weights[feature] * feature_scores[feature]\n",
    "\n",
    "    # Enumerate and rank by final score\n",
    "    sim_scores = list(enumerate(final_similarity))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    top_indices = [i for i, _ in sim_scores if i != book_index][:top_n]\n",
    "    return top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7eab74b-af70-4c46-b0d7-f9285a9ff7b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "hits = 0\n",
    "total = 0\n",
    "precision_total = 0\n",
    "recall_total = 0\n",
    "\n",
    "for user, group in tqdm(ratings_filtered.groupby('userID')):\n",
    "    user_books = group[group['bookRating'] >= 5]['ISBN'].tolist()\n",
    "    \n",
    "    if len(user_books) < 6:\n",
    "        continue\n",
    "\n",
    "    for seed_book in user_books:\n",
    "        other_liked_books = [b for b in user_books if b != seed_book]\n",
    "        if not other_liked_books:\n",
    "            continue\n",
    "        \n",
    "        book_index = top_books[top_books['ISBN'] == seed_book].index[0]\n",
    "        recommended_books = recommend_indices_intepretable(book_index, similarity_matrices_extra, similarity_weights_extra, top_books, 10)\n",
    "        # get ISBN of recommended books\n",
    "        recommended_books = top_books.loc[recommended_books, 'ISBN'].tolist()\n",
    "        # True positives\n",
    "        relevant_recs = set(recommended_books).intersection(set(other_liked_books))\n",
    "        hits += int(len(relevant_recs) > 0)\n",
    "        precision_total += len(relevant_recs) / top_n\n",
    "        recall_total += len(relevant_recs) / len(other_liked_books)\n",
    "        total += 1\n",
    "\n",
    "# Metrics\n",
    "print(f\"Hit Rate@{top_n}: {hits / total:.4f}\")\n",
    "print(f\"Precision@{top_n}: {precision_total / total:.4f}\")\n",
    "print(f\"Recall@{top_n}: {recall_total / total:.4f}\")\n",
    "\n",
    "hit_rate_google_api = hits / total\n",
    "precision_google_api = precision_total / total\n",
    "recall_google_api = recall_total / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e2210ad-719f-4854-a0b5-6b97e451fbab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This yields the largest increase in performance so far and has a hit rate of over 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813b6c8c-c812-48da-a39a-1ffbe4ae0f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "models = ['Popularity model', 'Base features', 'Base features + title', 'With Google API features']\n",
    "\n",
    "hit_rate = [hit_rate_baseline, hit_rate_base_features, hit_rate_with_title, hit_rate_google_api]\n",
    "precision = [precision_baseline, precision_base_features, precision_with_title, precision_google_api]\n",
    "recall = [recall_baseline, recall_base_features, recall_with_title, recall_google_api]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "axes[0].bar(models, hit_rate, color='skyblue')\n",
    "axes[0].set_title('Hit Rate')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_xticklabels(models, rotation=45, ha='right')\n",
    "\n",
    "axes[1].bar(models, precision, color='lightgreen')\n",
    "axes[1].set_title('Precision')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].set_xticklabels(models, rotation=45, ha='right')\n",
    "\n",
    "axes[2].bar(models, recall, color='salmon')\n",
    "axes[2].set_title('Recall')\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].set_xticklabels(models, rotation=45, ha='right')\n",
    "\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dbd4eb8-ba52-4eeb-a166-4909a9dff940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Using the features from the google API including genre and description, I am able to marginally improve the result. I expect learning the weights of the different factors would improve the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9efe928c-43cb-44bd-8285-6c734d6dbf41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualising the recommendation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f5f68a9-09bb-4735-b3ee-5254c104bb92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "book_index = 4\n",
    "print(f\"Book : {top_books.iloc[book_index]}\")\n",
    "recommendations_example = recommend_books_interpretable(book_index, similarity_matrices_extra, similarity_weights_extra, top_books, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6755f41-f8b0-4475-8e4b-86eda8a5648f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "similarity_columns = ['sim_title', 'sim_author', 'sim_publisher', 'sim_age', 'sim_description', 'sim_category', 'sim_pages']\n",
    "data = recommendations_example.iloc[0][similarity_columns]\n",
    "labels = similarity_columns\n",
    "values = data.values.flatten().tolist()\n",
    "values += values[:1]\n",
    "\n",
    "num_vars = len(labels)\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "ax.plot(angles, values, color='red', linewidth=2)\n",
    "ax.fill(angles, values, color='red', alpha=0.25)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(labels, fontsize=10)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.title('Similarity Scores Radar Chart', size=16, y=1.1)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7c243d-203d-4123-9645-bfd606f8872f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "weights = {\n",
    "    'sim_title': 0.15,\n",
    "    'sim_author': 0.05,\n",
    "    'sim_publisher': 0,\n",
    "    'sim_age': 0.1,\n",
    "    'sim_description': 0.5,\n",
    "    'sim_category': 0.1,\n",
    "    'sim_pages': 0.1\n",
    "}\n",
    "\n",
    "similarity_columns = ['sim_title', 'sim_author', 'sim_publisher', 'sim_age', 'sim_description', 'sim_category', 'sim_pages']\n",
    "columns = [col for col in similarity_columns if weights[col] > 0]\n",
    "raw_scores = recommendations_example.iloc[0][columns]\n",
    "weighted_components = raw_scores * np.array([weights[col] for col in columns])\n",
    "max_val = max(weighted_components.max(), 1e-6)\n",
    "normalized_components = weighted_components / max_val\n",
    "values = normalized_components.tolist()\n",
    "values += values[:1]\n",
    "num_vars = len(columns)\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "book_title = recommendations_example.iloc[0]['bookTitle']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "ax.plot(angles, values, color='purple', linewidth=2, label=book_title)\n",
    "ax.fill(angles, values, color='purple', alpha=0.25)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(columns, fontsize=10)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))\n",
    "plt.title('Weighted Similarity Components', size=16, y=1.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34206bd5-e113-40d2-9d62-a38100010d3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "weights = {\n",
    "    'sim_title': 0.15,\n",
    "    'sim_author': 0.05,\n",
    "    'sim_publisher': 0,\n",
    "    'sim_age': 0.1,\n",
    "    'sim_description': 0.5,\n",
    "    'sim_category': 0.1,\n",
    "    'sim_pages': 0.1\n",
    "}\n",
    "\n",
    "similarity_columns = ['sim_title', 'sim_author', 'sim_publisher', 'sim_age', 'sim_description', 'sim_category', 'sim_pages']\n",
    "columns = [col for col in similarity_columns if weights[col] > 0]\n",
    "\n",
    "\n",
    "num_vars = len(columns)\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "for idx in range(5):\n",
    "    raw_scores = recommendations_example.iloc[idx][columns]\n",
    "    weighted_components = raw_scores * np.array([weights[col] for col in columns])\n",
    "    max_val = max(weighted_components.max(), 1e-6)\n",
    "    normalized_components = weighted_components / max_val\n",
    "    \n",
    "    values = normalized_components.tolist()\n",
    "    values += values[:1]\n",
    "\n",
    "    book_title = recommendations_example.iloc[idx]['bookTitle']\n",
    "    \n",
    "    ax.plot(angles, values, linewidth=2, label=book_title)\n",
    "    ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(columns, fontsize=10)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.2, 1.05), fontsize=9)\n",
    "\n",
    "plt.title('Top 5 Weighted Similarity Components', size=16, y=1.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "835ddf9d-067b-477c-8c67-5c03b878093f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "It would also be nice to have some variety measure maybe. So that all the recommendations aren't the same in the same way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b2cc04-6d46-4734-bbb1-3bf30ccf4b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Now all in one with the recommendations and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55b74f1-26ec-47c7-84ba-9f75faf50778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def recommend_and_plot_radar(book_index, similarity_matrices_extra, similarity_weights_extra, top_books_extra_no_null, top_n=5):\n",
    "    recommendations_example = recommend_books_interpretable(\n",
    "        book_index,\n",
    "        similarity_matrices_extra,\n",
    "        similarity_weights_extra,\n",
    "        top_books_extra_no_null,\n",
    "        top_n\n",
    "    )\n",
    "\n",
    "    print(recommendations_example[['bookTitle', 'Score']])\n",
    "    \n",
    "    similarity_columns = list(similarity_weights_extra.keys())\n",
    "    columns = [col for col in similarity_columns if similarity_weights_extra[col] > 0]\n",
    "    columns = ['sim_' + col for col in columns]\n",
    "    \n",
    "    num_vars = len(columns)\n",
    "    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    for idx in range(min(top_n, len(recommendations_example))):\n",
    "        raw_scores = recommendations_example.iloc[idx][columns]\n",
    "\n",
    "        weighted_components = raw_scores * np.array([similarity_weights_extra[col.split('_')[-1]] for col in columns])\n",
    "\n",
    "        max_val = max(weighted_components.max(), 1e-6)\n",
    "        normalized_components = weighted_components / max_val\n",
    "        \n",
    "        values = normalized_components.tolist()\n",
    "        values += values[:1]\n",
    "        \n",
    "        book_title = recommendations_example.iloc[idx]['bookTitle']\n",
    "        \n",
    "        ax.plot(angles, values, linewidth=2, label=book_title)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(columns, fontsize=10)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1.2, 1.05), fontsize=9)\n",
    "    \n",
    "    plt.title(f\"Top {top_n} Books' Weighted Similarity Components\", size=16, y=1.1)\n",
    "    plt.show()\n",
    "    \n",
    "    return recommendations_example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "867d8d9a-6665-4ff3-b1ea-80610d687685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "similarity_weights_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b41ba45-83f9-49c5-b820-8c4525014907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_books.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31a50816-47e1-42ff-bd82-68b84045a773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "recommended_books = recommend_and_plot_radar(\n",
    "    4,\n",
    "    similarity_matrices_extra,\n",
    "    similarity_weights_extra,\n",
    "    top_books,\n",
    "    top_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7884c4-50b3-49c1-bb32-c9ca59eed91d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I recommend books using the default similarity measures I initially defined. I decide I care most about the description and less about author, publisher and age and get new recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32ab306-9e7b-40da-b767-2889d7725659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'title': 0.15,\n",
    "    'author': 0.05,\n",
    "    'publisher': 0.05,\n",
    "    'age': 0.05,\n",
    "    'description': 0.5,\n",
    "    'category': 0.1,\n",
    "    'pages': 0.1\n",
    "}\n",
    "\n",
    "recommended_books = recommend_and_plot_radar(\n",
    "    4,\n",
    "    similarity_matrices_extra,\n",
    "    weights,\n",
    "    top_books,\n",
    "    top_n=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3b51994-3a76-4111-b9b8-40f3482f1993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lets give a Collaborative Filtering approach a go\n",
    "\n",
    "I will go for an item-based over user-based collaborative filtering approach  This is best since I am assuming no user history only that they read this one book and would like some similar recommendations and it would be difficult to find similar users based on just one book. There will likely be many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bbc04ce2-8f93-4d71-999b-087ba19e05f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# I will filter down for just explicit ratings for now\n",
    "ratings_filtered_explicit = ratings_df[ratings_df['bookRating'] > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95be28bb-5f7d-475b-a4d0-1d750650bcc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'user_id': [1, 1, 2, 2, 3, 3],\n",
    "    'item_id': ['A', 'B', 'A', 'C', 'B', 'C'],\n",
    "    'rating': [5, 3, 4, 2, 2, 5]\n",
    "}\n",
    "\n",
    "user_item_matrix = ratings_filtered_explicit.pivot_table(index='userID', columns='ISBN', values='bookRating')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33f94a17-1f4d-40a6-8e35-ab0dc3805660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# lets go for cosine similarity again\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "user_item_matrix = user_item_matrix.T.fillna(0)\n",
    "similarity_matrix_item_cf = cosine_similarity(user_item_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07bc8d01-f134-4a97-94fb-65f77eb1d21e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cf_item_similarity_df = pd.DataFrame(similarity_matrix_item_cf, \n",
    "                                   index=user_item_matrix.index, \n",
    "                                   columns=user_item_matrix.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "749d9ce0-6005-48ed-b9bf-5cfb45c54008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def recommend_similar_books(liked_book, similarity_matrix, books_df, top_n=5):\n",
    "    if liked_book not in similarity_matrix.index:\n",
    "        raise ValueError(f\"Book '{liked_book}' not found in similarity matrix.\")\n",
    "    similar_books = similarity_matrix.loc[liked_book].drop(labels=[liked_book])\n",
    "    top_recommendations = similar_books.sort_values(ascending=False).head(top_n)\n",
    "    top_recommendations = books_df[books_df.ISBN.isin(top_recommendations.index)]\n",
    "    return top_recommendations\n",
    "\n",
    "def recommend_similar_books_isbn(liked_book, similarity_matrix, books_df, top_n=5):\n",
    "    if liked_book not in similarity_matrix.index:\n",
    "        raise ValueError(f\"Book '{liked_book}' not found in similarity matrix.\")\n",
    "    similar_books = similarity_matrix.loc[liked_book].drop(labels=[liked_book])\n",
    "    top_recommendations = similar_books.sort_values(ascending=False).head(top_n)\n",
    "    return top_recommendations.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "481ade1e-cb42-48fa-998d-0801dd80f88e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "book_index = 4\n",
    "book_isbn = top_books.iloc[book_index]['ISBN']\n",
    "print(f\"Recommendations for: {top_books.iloc[book_index]}\")\n",
    "recommend_similar_books(book_isbn, cf_item_similarity_df, top_books, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99393cd3-5d74-414b-9996-ebb0f5ad12ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "book_index = 5\n",
    "book_isbn = top_books.iloc[book_index]['ISBN']\n",
    "print(f\"Recommendations for: {top_books.iloc[book_index]}\")\n",
    "recommend_similar_books(book_isbn, cf_item_similarity_df, top_books, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba43a4b-1b6a-42b0-8861-63e1b2c7d045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Quick Evaluation of user-based CF on its own\n",
    "\n",
    "Not I will use the same method of evaluation I designed and used for the content based recommender - however this is likely at risk of some data leakage as the ratings was used to construct the similarity matrix. And these results might be inflated slightly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e7ec010-c832-4716-a386-20386d5e8d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "hits = 0\n",
    "total = 0\n",
    "precision_total = 0\n",
    "recall_total = 0\n",
    "\n",
    "for user, group in tqdm(ratings_filtered.groupby('userID')):\n",
    "    user_books = group[group['bookRating'] >= 5]['ISBN'].tolist()\n",
    "    \n",
    "    if len(user_books) < 6:\n",
    "        continue\n",
    "\n",
    "    for seed_book in user_books:\n",
    "        other_liked_books = [b for b in user_books if b != seed_book]\n",
    "        if not other_liked_books:\n",
    "            continue\n",
    "        \n",
    "        book_index = top_books[top_books['ISBN'] == seed_book].index[0]\n",
    "        recommended_books = recommend_similar_books_isbn(seed_book, cf_item_similarity_df, top_books, top_n=top_n)\n",
    "        # True positives\n",
    "        relevant_recs = set(recommended_books).intersection(set(other_liked_books))\n",
    "        hits += int(len(relevant_recs) > 0)\n",
    "        precision_total += len(relevant_recs) / top_n\n",
    "        recall_total += len(relevant_recs) / len(other_liked_books)\n",
    "        total += 1\n",
    "\n",
    "# Metrics\n",
    "print(f\"Hit Rate@{top_n}: {hits / total:.4f}\")\n",
    "print(f\"Precision@{top_n}: {precision_total / total:.4f}\")\n",
    "print(f\"Recall@{top_n}: {recall_total / total:.4f}\")\n",
    "\n",
    "hit_rate_google_api = hits / total\n",
    "precision_google_api = precision_total / total\n",
    "recall_google_api = recall_total / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83983103-03b3-433c-9e3f-975c56d8f2c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Well it does seem like this model is... well there's no possibility to beat around the bush here, much better than the content based one I built - I could claim that this evaluation unfairly benefits the CF model. I would like to further investigate if adding some semantic ability to my content based one would increase the performance a bit but maybe what I can do is try and combine them. I would also try and find a fairer evaluation method to compare both - maybe for now I could investigate calculating the similarity matrix in each iteration without the given users data as for rarer books this could be quite influential.\n",
    "\n",
    "Another caveat is that I have sculpted the dataset to be more in the domain of a Collaborative Filtering model and that the content based model is more useful for these item cold start cases where we aren't dealing with popular books with lots of user data! So this evaluation is very rather harsh and a qualitative evaluation and user testing could prove that the content-based model does perfom okay. \n",
    "\n",
    "Diversity of recommendations has also not been considered as an option too! Sometimes a book reader wouldn't mind a bit of a wild card suggestion or at least variety of options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24704ef6-4d17-42b6-afe6-30bada05e7dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Combining collaborative filtering and content-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae043fb-28ec-4af4-add5-0907da0ae66d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Since the very simple collaborative filtering outperformed my content-based model. I will combine them so that collaborative filtering gives the recommendations say 100 and the content-based will rerank them and be used to explain in a way why they are relevant with the caveat that they were recommended because other readers that liked this book liked these other books too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c46d9cd-25cb-43c7-a1a0-bb8c37dc436f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def recommend_indices_interpretable_subset(\n",
    "    book_index, \n",
    "    candidate_indices, \n",
    "    similarity_matrices, \n",
    "    weights, \n",
    "    top_n=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Re-ranks a list of candidate book indices using weighted content-based similarity\n",
    "    to a given book_index.\n",
    "\n",
    "    Args:\n",
    "        book_index (int): Index of the seed (liked) book.\n",
    "        candidate_indices (list[int]): List of book indices to re-rank.\n",
    "        similarity_matrices (dict): Dictionary of similarity matrices per feature.\n",
    "        weights (dict): Corresponding weights for each similarity matrix.\n",
    "        top_n (int): Number of top recommendations to return.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: Top-n indices from candidate_indices, re-ranked by weighted similarity.\n",
    "    \"\"\"\n",
    "    feature_scores = {\n",
    "        feature: sim_matrix[book_index] for feature, sim_matrix in similarity_matrices.items()\n",
    "    }\n",
    "\n",
    "    final_scores = {}\n",
    "    for idx in candidate_indices:\n",
    "        score = sum(weights[feature] * feature_scores[feature][idx] for feature in similarity_matrices)\n",
    "        final_scores[idx] = score\n",
    "\n",
    "    # Sort by score and return top_n indices\n",
    "    sorted_scores = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_indices = [idx for idx, _ in sorted_scores[:top_n]]\n",
    "    return top_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cdb1fee-bc8f-4424-9433-040f4d0b16d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cf_top_n = 50\n",
    "top_n = 5\n",
    "hits = 0\n",
    "total = 0\n",
    "precision_total = 0\n",
    "recall_total = 0\n",
    "\n",
    "\n",
    "# use these weights for content based - same as for previous recommendation\n",
    "content_based_weights = {'title': 0.1, 'author': 0.2, 'publisher': 0.1, 'age': 0.05, 'description': 0.2, 'category': 0.2, 'pages': 0.05}\n",
    "\n",
    "# original similarity matrix too\n",
    "content_based_similarity_matrices = similarity_matrices_extra\n",
    "\n",
    "for user, group in tqdm(ratings_filtered.groupby('userID')):\n",
    "    user_books = group[group['bookRating'] >= 5]['ISBN'].tolist()\n",
    "    if len(user_books) < 2:\n",
    "        continue\n",
    "\n",
    "    for seed_book in user_books:\n",
    "        other_liked_books = [b for b in user_books if b != seed_book]\n",
    "        if not other_liked_books:\n",
    "            continue\n",
    "        \n",
    "        book_index = top_books[top_books['ISBN'] == seed_book].index[0]\n",
    "        recommended_books_cf = recommend_similar_books_isbn(seed_book, cf_item_similarity_df, top_books, top_n=cf_top_n)\n",
    "        cf_candidate_indices = [top_books[top_books['ISBN'] == isbn].index[0] for isbn in recommended_books_cf if isbn in top_books['ISBN'].values]\n",
    "        re_ranked_content_based_indices = recommend_indices_interpretable_subset(\n",
    "            book_index, \n",
    "            cf_candidate_indices, \n",
    "            content_based_similarity_matrices, \n",
    "            content_based_weights, \n",
    "            top_n=top_n\n",
    "        )\n",
    "        recommended_books = top_books.iloc[re_ranked_content_based_indices]['ISBN'].tolist()\n",
    "        # True positives\n",
    "        relevant_recs = set(recommended_books).intersection(set(other_liked_books))\n",
    "        hits += int(len(relevant_recs) > 0)\n",
    "        precision_total += len(relevant_recs) / top_n\n",
    "        recall_total += len(relevant_recs) / len(other_liked_books)\n",
    "        total += 1\n",
    "\n",
    "# Metrics\n",
    "print(f\"Hit Rate@{top_n}: {hits / total:.4f}\")\n",
    "print(f\"Precision@{top_n}: {precision_total / total:.4f}\")\n",
    "print(f\"Recall@{top_n}: {recall_total / total:.4f}\")\n",
    "\n",
    "hit_rate_google_api = hits / total\n",
    "precision_google_api = precision_total / total\n",
    "recall_google_api = recall_total / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4efc0b46-e74e-43d6-a612-abe3cf088322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Combining the collaborative filtering and content based models in this way does not improve the metrics but it does improve upon just using the content based one on its own. I still think the content based model can be valuable for the recommender system as it can give the user some explanations and details about why it is relevant and allow them to feedback into it. It may also be better in the cold start situation where the book is not so popular."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_recommender",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
